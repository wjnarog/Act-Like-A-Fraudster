{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c894032d-ae3a-49fb-87e6-22cad7890e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\zach0\\anaconda3\\lib\\site-packages (4.26.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from selenium) (0.27.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\zach0\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\zach0\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zach0\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install beautifulsoup4\n",
    "!pip install pandas\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "546d6449-d160-46c7-a019-efefc4f1456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set0 = {}\n",
    "data_set1 = {}\n",
    "data_set2 = {}\n",
    "data_set3 = {}\n",
    "data_set4 = {}\n",
    "data_set5 = {}\n",
    "data_set6 = {}\n",
    "data_set7 = {}\n",
    "data_set8 = {}\n",
    "data_set9 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91b47234-44b4-4bf7-9128-9551b5b851bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: \n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m driver\u001b[38;5;241m.\u001b[39mimplicitly_wait(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Wait until the table rows are loaded (anchor tags with 'table-row' class)\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m WebDriverWait(driver, \u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(\n\u001b[0;32m     27\u001b[0m     EC\u001b[38;5;241m.\u001b[39mpresence_of_all_elements_located((By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma.table-row\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Locate the first row (anchor tag with class 'table-row') and click it\u001b[39;00m\n\u001b[0;32m     31\u001b[0m first_row \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma.table-row\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:105\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll)\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://apps.douglas.co.us/assessor/web#/\")\n",
    "data_set = {}\n",
    "\n",
    "# Wait for the search box to be visible and locate it\n",
    "search_box = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.CSS_SELECTOR, 'app-input-debounce input[type=\"text\"]'))\n",
    ")\n",
    "\n",
    "# Send search query\n",
    "search_box.send_keys(\"1803 Lake Drive\")\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for the results to load (you may need to adjust this depending on the page)\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "# Wait until the table rows are loaded (anchor tags with 'table-row' class)\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a.table-row'))\n",
    ")\n",
    "\n",
    "# Locate the first row (anchor tag with class 'table-row') and click it\n",
    "first_row = driver.find_element(By.CSS_SELECTOR, 'a.table-row')\n",
    "first_row.click()\n",
    "\n",
    "# Get page source to parse the HTML after clicking the first row\n",
    "page_source = driver.page_source\n",
    "\n",
    "# BeautifulSoup to parse the HTML for further scraping\n",
    "soup = BeautifulSoup(page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79dc6db9-8010-4d52-a07c-a1a258554f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the pop-up dialog to appear and then click the \"Close\" button\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# Wait for the \"Close\" button to be clickable\n",
    "close_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button/span[text()='Close']\")))\n",
    "\n",
    "# Click the \"Close\" button to dismiss the pop-up\n",
    "close_button.click()\n",
    "\n",
    "# Wait for the account summary section to be loaded\n",
    "wait.until(EC.presence_of_element_located((By.XPATH, \"//div[@class='dropdown-content']\")))\n",
    "\n",
    "# Extract HTML content\n",
    "html_content = driver.page_source\n",
    "\n",
    "# Use BeautifulSoup to parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "165077ef-5cc6-43e3-92ae-8182479fea69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account Summary:\n",
      "Account #:: R0396757\n",
      "State Parcel #:: 2607-202-03-017\n",
      "Account Type:: Residential\n",
      "Tax District:: 0169\n",
      "Neighborhood-Ext:: 605-B\n",
      "Building Count:: 1\n",
      "Building Permit Authority:: Douglas County (website )\n",
      "Phone:: 303-660-7497\n",
      "Name:: SAGE PORT\n",
      "Reception No:: 9612831\n",
      "\n",
      "Location Description: LOT 66 SAGE PORT FILING #4 FIRST AMENDMENT. 2.30 AM/L\n",
      "\n",
      "Owner Info:\n",
      "Owner Name: COOK FAMILY TRUST\n",
      "Owner Address: 1803 LAKE DRLARKSPUR, CO 80118\n",
      "\n",
      "Public Land Survey System (PLSS) Location: \n",
      "Quarter: NW; \n",
      "Section: 20; \n",
      "Township: 9; \n",
      "Range: 67\n",
      "\n",
      "Section PDF Map Link: /realware/SectionMaps/TWP2607/DC_2607_20.pdf\n",
      "{'Account #:': 'R0396757', 'State Parcel #:': '2607-202-03-017', 'Account Type:': 'Residential', 'Tax District:': '0169', 'Neighborhood-Ext:': '605-B', 'Building Count:': '1', 'Building Permit Authority:': 'Douglas County (website\\xa0)', 'Phone:': '303-660-7497', 'Name:': 'SAGE PORT', 'Reception No:': '9612831', 'Location Description': 'LOT 66 SAGE PORT FILING #4 FIRST AMENDMENT. 2.30 AM/L', 'Owner Name': 'COOK FAMILY TRUST', 'Owner Address': '1803 LAKE DRLARKSPUR, CO 80118'}\n"
     ]
    }
   ],
   "source": [
    "# Extract Toggle Button and Links before Account Summary\n",
    "html_content = driver.page_source\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Initialize a dictionary to store toggle button and links\n",
    "key_value_pairs = {}\n",
    "\n",
    "# Extract the toggle button text (key) and status (value)\n",
    "toggle_button = soup.find('span', class_='ui-button-text')\n",
    "if toggle_button:\n",
    "    key_value_pairs[\"Toggle Button\"] = toggle_button.text.strip()\n",
    "\n",
    "# Extract the anchor tags (links) and their href attributes\n",
    "links = soup.find_all('a', href=True)\n",
    "for link in links:\n",
    "    link_text = link.get_text(strip=True)\n",
    "    link_url = link['href']\n",
    "    key_value_pairs[link_text] = f'<a href=\"{link_url}\">{link_text}</a>'\n",
    "\n",
    "# Now proceed with the Account Summary logic\n",
    "# Target the dropdown for Account Summary by using its ID\n",
    "dropdown_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[@id='SummaryAccountInfo']//span[@class='bar faux-button']\")))\n",
    "\n",
    "# Click the dropdown to expand it\n",
    "dropdown_button.click()\n",
    "\n",
    "# Wait for the Account Summary content to load\n",
    "wait.until(EC.presence_of_element_located((By.XPATH, \"//div[@id='SummaryAccountInfo']//div[@class='dropdown-content']\")))\n",
    "\n",
    "# Extract HTML content again after the dropdown is expanded\n",
    "html_content = driver.page_source\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the dropdown content specifically under the 'SummaryAccountInfo' ID\n",
    "dropdown_content = soup.find('div', id='SummaryAccountInfo').find('div', class_='dropdown-content')\n",
    "\n",
    "# Extract the key-value pairs from Account Summary\n",
    "account_summary_pairs = {}\n",
    "\n",
    "# Find all the rows with the class 'skinny-row' which contain the label and value pairs\n",
    "rows = dropdown_content.find_all('div', class_='skinny-row')\n",
    "\n",
    "for row in rows:\n",
    "    # Extract the label (key) and value (value)\n",
    "    label = row.find('div', class_='col-xs-4').text.strip() if row.find('div', class_='col-xs-4') else None\n",
    "    value = row.find('div', class_='col-xs-8').text.strip() if row.find('div', class_='col-xs-8') else None\n",
    "    \n",
    "    # Add to dictionary if both label and value exist\n",
    "    if label and value:\n",
    "        account_summary_pairs[label] = value\n",
    "\n",
    "# Print the Account Summary key-value pairs\n",
    "print(f\"Account Summary:\")\n",
    "for key, value in account_summary_pairs.items():\n",
    "    # Skip the \"Update Mailing Address\" entry\n",
    "    if key != 'Update Mailing Address':\n",
    "        print(f\"{key}: {value}\")\n",
    "        data_set0[key] = value\n",
    "\n",
    "# Extract additional data (Location Description, Owner Info, PLSS Location)\n",
    "location_description = soup.find('div', string='Location Description').find_next('div').text.strip()\n",
    "\n",
    "# For Owner Info, we need to extract and clean it\n",
    "owner_info_div = soup.find('div', string='Owner Info').find_next('div')\n",
    "\n",
    "# Extract owner name and address\n",
    "owner_info_raw = owner_info_div.text.strip()\n",
    "\n",
    "# Split owner info into lines\n",
    "owner_info_parts = owner_info_raw.split(\"\\n\")\n",
    "\n",
    "# Clean up and extract the name and address properly\n",
    "owner_name = owner_info_parts[0].strip() \n",
    "owner_address = \" \".join(owner_info_parts[1:]).strip()  \n",
    "\n",
    "# If \"Update Mailing Address\" appears in the address, remove it\n",
    "if \"Update Mailing Address\" in owner_address:\n",
    "    owner_address = owner_address.split(\"Update Mailing Address\")[0].strip()\n",
    "\n",
    "# Extract PLSS Location\n",
    "plss_location = soup.find('div', string='Public Land Survey System (PLSS) Location').find_next('div').text.strip()\n",
    "\n",
    "# Clean the PLSS Location\n",
    "plss_location_cleaned = ' '.join(plss_location.split())\n",
    "\n",
    "# Optionally, reformat for better readability (if you want to format it neatly)\n",
    "plss_location_cleaned = plss_location_cleaned.replace(\"Quarter:\", \"\\nQuarter:\").replace(\"Section:\", \"\\nSection:\").replace(\"Township:\", \"\\nTownship:\").replace(\"Range:\", \"\\nRange:\")\n",
    "\n",
    "# Extract the Section PDF Map link (if it exists)\n",
    "section_pdf_map = None\n",
    "\n",
    "# Find all the div elements with class 'skinny-row'\n",
    "pdf_map_rows = soup.find_all('div', class_='skinny-row')\n",
    "\n",
    "for row in pdf_map_rows:\n",
    "    # Look for an anchor tag within the row\n",
    "    link = row.find('a', href=True)\n",
    "    if link and \"SectionMap\" in link['href']:  # Check if the href contains \"SectionMap\"\n",
    "        section_pdf_map = link['href']\n",
    "        break \n",
    "\n",
    "# Print other extracted information\n",
    "print(f\"\\nLocation Description: {location_description}\")\n",
    "data_set0[\"Location Description\"] = location_description\n",
    "print(f\"\\nOwner Info:\")\n",
    "print(f\"Owner Name: {owner_name}\")\n",
    "data_set0[\"Owner Name\"] = owner_name\n",
    "print(f\"Owner Address: {owner_address}\")\n",
    "data_set0[\"Owner Address\"] = owner_address\n",
    "print(f\"\\nPublic Land Survey System (PLSS) Location: {plss_location_cleaned}\")\n",
    "#data_set0[\"PLSS Location\"] = {plss_location_cleaned}\n",
    "print(f\"\\nSection PDF Map Link: {section_pdf_map}\")\n",
    "#data_set0[\"PDF Map Link\"] = {section_pdf_map}\n",
    "\n",
    "print(data_set0)\n",
    "\n",
    "# Close the browser\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f994a4bd-4c36-4c6c-ac44-86cfefde7ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show graphs\n",
      "{'Get Taxes Due': 'http://apps.douglas.co.us/treasurer/treasurerweb/account.jsp?account=R0396757&guest=true', 'Property Tax Calculation': 'https://www.douglas.co.us/assessor/residential-property-tax-calculations', 'sale0': {'Year': '2024', 'Actual Value': '$1,295,427', 'Assessed Value': '$86,790', 'Tax Rate': '9.7470%', 'Est. Tax Amount': 'Tax Calculation'}, 'sale1': {'Year': '2023', 'Actual Value': '$1,295,427', 'Assessed Value': '$86,790', 'Tax Rate': '9.8441%', 'Est. Tax Amount': 'Tax Calculation'}, 'sale2': {'Year': '2022', 'Actual Value': '$852,669', 'Assessed Value': '$59,260', 'Tax Rate': '9.4724%', 'Est. Tax Amount': '$5,613'}, 'sale3': {'Year': '2021', 'Actual Value': '$852,669', 'Assessed Value': '$60,970', 'Tax Rate': '9.5310%', 'Est. Tax Amount': '$5,811'}, 'sale4': {'Year': '2020', 'Actual Value': '$743,866', 'Assessed Value': '$53,190', 'Tax Rate': '9.6060%', 'Est. Tax Amount': '$5,109'}, 'sale5': {'Year': '2019', 'Actual Value': '$743,866', 'Assessed Value': '$53,190', 'Tax Rate': '9.6378%', 'Est. Tax Amount': '$5,126'}, 'sale6': {'Year': '2018', 'Actual Value': '$659,857', 'Assessed Value': '$47,510', 'Tax Rate': '9.8102%', 'Est. Tax Amount': '$4,661'}, 'sale7': {'Year': '2017', 'Actual Value': '$659,857', 'Assessed Value': '$47,510', 'Tax Rate': '9.2206%', 'Est. Tax Amount': '$4,381'}, 'sale8': {'Year': '2016', 'Actual Value': '$611,328', 'Assessed Value': '$48,670', 'Tax Rate': '9.3750%', 'Est. Tax Amount': '$4,563'}, 'sale9': {'Year': '2015', 'Actual Value': '$611,328', 'Assessed Value': '$48,670', 'Tax Rate': '9.1681%', 'Est. Tax Amount': '$4,462'}, 'sale10': {'Year': '2014', 'Actual Value': '$528,657', 'Assessed Value': '$42,080', 'Tax Rate': '9.7768%', 'Est. Tax Amount': '$4,114'}, 'sale11': {'Year': '2013', 'Actual Value': '$528,657', 'Assessed Value': '$42,080', 'Tax Rate': '9.7798%', 'Est. Tax Amount': '$4,115'}, 'sale12': {'Year': '2012', 'Actual Value': '$579,914', 'Assessed Value': '$46,160', 'Tax Rate': '9.8364%', 'Est. Tax Amount': '$4,540'}, 'sale13': {'Year': '2011', 'Actual Value': '$579,914', 'Assessed Value': '$46,160', 'Tax Rate': '9.8265%', 'Est. Tax Amount': '$4,536'}, 'sale14': {'Year': '2010', 'Actual Value': '$819,982', 'Assessed Value': '$65,270', 'Tax Rate': '9.4128%', 'Est. Tax Amount': '$6,144'}, 'sale15': {'Year': '2009', 'Actual Value': '$819,982', 'Assessed Value': '$65,270', 'Tax Rate': '9.3955%', 'Est. Tax Amount': '$6,132'}, 'sale16': {'Year': '2008', 'Actual Value': '$799,442', 'Assessed Value': '$63,640', 'Tax Rate': '9.4639%', 'Est. Tax Amount': '$6,023'}, 'sale17': {'Year': '2007', 'Actual Value': '$799,442', 'Assessed Value': '$63,640', 'Tax Rate': '9.6944%', 'Est. Tax Amount': '$6,170'}, 'sale18': {'Year': '2006', 'Actual Value': '$794,256', 'Assessed Value': '$63,230', 'Tax Rate': '9.8685%', 'Est. Tax Amount': '$6,240'}}\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store toggle button and links\n",
    "key_value_pairs = {}\n",
    "\n",
    "# Extract the toggle button text (Show Graphs)\n",
    "toggle_button = soup.find('span', class_='ui-button-text')\n",
    "if toggle_button:\n",
    "    print(toggle_button.text.strip())  # Print 'Show Graphs'\n",
    "\n",
    "# Extract the anchor tags (links) and their href attributes\n",
    "links = soup.find_all('a', href=True)\n",
    "for link in links:\n",
    "    link_text = link.get_text(strip=True)\n",
    "    link_url = link['href']\n",
    "    \n",
    "    # Print specific links that we are interested in\n",
    "    if link_text == \"Get Taxes Due\":\n",
    "        data_set1[link_text] = link_url  # Hyperlink Get Taxes Due\n",
    "    elif link_text == \"Property Tax Calculation\":\n",
    "        data_set1[link_text] = link_url  # Hyperlink Property Tax Calculation\n",
    "\n",
    "# Now continue with the part for finding and processing the table data\n",
    "# Find the table with class 'value-data-table'\n",
    "table = soup.find('table', class_='value-data-table')\n",
    "if table:\n",
    "    # print(\"Table found\")  \n",
    "\n",
    "    # Find all tbody elements inside the table with the 'sales-info' class, without specifying the dynamic part\n",
    "    rows = table.find_all('tbody', class_='value-row')  \n",
    "\n",
    "    # Check how many rows are found\n",
    "    # print(f\"Found {len(rows)} tbody elements.\")  \n",
    "\n",
    "    # Iterate over each row and extract data\n",
    "    sales_data = []\n",
    "    for row in rows:\n",
    "        # Extract Year, Actual Value, Assessed Value, Tax Rate, Est. Tax Amount\n",
    "        year = row.find_all('td')[0].text.strip() if len(row.find_all('td')) > 1 else None\n",
    "        actual_value = row.find_all('td')[1].text.strip() if len(row.find_all('td')) > 1 else None\n",
    "        assesssed_value = row.find_all('td')[2].text.strip() if len(row.find_all('td')) > 1 else None\n",
    "        tax_rate = row.find_all('td')[3].text.strip() if len(row.find_all('td')) > 1 else None\n",
    "        est_tax_amount = row.find_all('td')[4].text.strip() if len(row.find_all('td')) > 1 else None\n",
    "    \n",
    "        # Store row data in a table\n",
    "        sales_data.append({\n",
    "            'Year': year,\n",
    "            'Actual Value': actual_value,\n",
    "            'Assessed Value': assesssed_value,\n",
    "            'Tax Rate': tax_rate,\n",
    "            'Est. Tax Amount': est_tax_amount\n",
    "        })\n",
    "        \n",
    "    # Print extracted data\n",
    "    index = 0\n",
    "    for row in sales_data:\n",
    "        data_set1[\"sale\"+str(index)] = row\n",
    "        index+=1\n",
    "\n",
    "    print(data_set1)\n",
    "\n",
    "else:\n",
    "    print(\"Table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffd5f82c-4d49-448c-be04-1ff79de1dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the table with class 'sales-data-table table'\n",
    "table = soup.find('table', class_='sales-data-table table')\n",
    "# Extract the anchor tags (links) and their href attributes\n",
    "\n",
    "links = soup.find_all('a', href=True)\n",
    "# Use a set to track printed links, avoid duplications\n",
    "printed_links = set()  \n",
    "\n",
    "for link in links:\n",
    "    link_text = link.get_text(strip=True)\n",
    "    link_url = link['href']\n",
    "    \n",
    "    # Print specific links that we are interested in, only if not already printed\n",
    "    if link_text == \"View Neighborhood Sales\" and link_url not in printed_links:\n",
    "        data_set2[link_text] = link_url\n",
    "        printed_links.add(link_url)  # Mark this link as printed\n",
    "    elif link_text == \"Recorded Document Search\" and link_url not in printed_links:\n",
    "        data_set2[link_text] = link_url\n",
    "        printed_links.add(link_url)\n",
    "\n",
    "        \n",
    "if table:\n",
    "    # print(\"Table found\")  \n",
    "    \n",
    "    # Find all tbody elements inside the table with the 'sales-info' class, without specifying the dynamic part\n",
    "    rows = table.find_all('tbody', class_='sales-info')  \n",
    "    # print(f\"Found {len(rows)} tbody elements.\")  \n",
    "    \n",
    "    # Iterate over each row and extract data\n",
    "    sales_data = []\n",
    "    for row in rows:\n",
    "        # Extract Reception No, Sale Date, Sale Price, Deed Type, etc.\n",
    "        reception_no = row.find_all('td')[0].text.strip() if len(row.find_all('td')) > 0 else None\n",
    "        \n",
    "        sale_date = row.find_all('td')[1].text.strip() if len(row.find_all('td')) > 1 else None\n",
    "        sale_price = row.find_all('td')[2].text.strip() if len(row.find_all('td')) > 1 else None\n",
    "        deed_type = row.find_all('td')[3].text.strip() if len(row.find_all('td')) > 2 else None\n",
    "        \n",
    "        # Extract Grantor and Grantee\n",
    "        sales_details_row = row.find_next('tr', class_='sales-details')\n",
    "        if sales_details_row:\n",
    "            grantor_grantee_div = sales_details_row.find('div', class_='col-sm-9 col-xs-12')\n",
    "            if grantor_grantee_div:\n",
    "                grantor = grantor_grantee_div.find_all('span', class_='ng-star-inserted')[0].text.strip().replace('Grantor:', '').strip()\n",
    "                grantee = grantor_grantee_div.find_all('span', class_='ng-star-inserted')[1].text.strip().replace('Grantee:', '').strip()\n",
    "            else:\n",
    "                grantor, grantee = None, None\n",
    "        else:\n",
    "            grantor, grantee = None, None\n",
    "        \n",
    "        # Store row data in a dictionary\n",
    "        sales_data.append({\n",
    "            'Reception No': reception_no,\n",
    "            'Sale Date': sale_date,\n",
    "            'Sale Price': sale_price,\n",
    "            'Deed Type': deed_type,\n",
    "            'Grantor': grantor,\n",
    "            'Grantee': grantee\n",
    "        })\n",
    "        \n",
    "    # Print extracted data\n",
    "    index = 0\n",
    "    for row in sales_data:\n",
    "        data_set2[\"sales_doc\"+ str(index)] = row\n",
    "        index += 1\n",
    "        \n",
    "\n",
    "else:\n",
    "    print(\"Table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "884ba1e6-8161-4783-959c-b1196711f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_data = {}\n",
    "temp_data_set = {}\n",
    "# Find the building details section using its ID\n",
    "building_details = soup.find('div', {'id': 'BuildingDetails'})\n",
    "\n",
    "if building_details:\n",
    "    # Scrape building images\n",
    "    images = building_details.find_all('img', class_='bordered')\n",
    "    image_urls = [img['src'] for img in images if img.has_attr('src')]\n",
    "    building_data['Images'] = image_urls\n",
    "    \n",
    "    # Scrape building primary info (property type, year built, etc.)\n",
    "    building_info = building_details.find_all('div', class_='smart-table')\n",
    "    primary_info = {}\n",
    "\n",
    "    for info in building_info:\n",
    "        # Find the label and value pairs for each group\n",
    "        label_elements = info.find_all('div', recursive=False)\n",
    "        \n",
    "        # Ensure there are exactly two divs, one for the label and one for the value\n",
    "        if len(label_elements) == 2:\n",
    "            label = label_elements[0].text.strip().replace('\\n', '').replace('\\r', '')\n",
    "            key, new_value = label.split(\": \")\n",
    "            data_set3[key] = new_value.replace(' ', '')\n",
    "            value = label_elements[1].text.strip().replace('\\n', '').replace('\\r', '')\n",
    "            key, new_value = value.split(\":\")\n",
    "            data_set3[key] = new_value.replace(' ', '')\n",
    "\n",
    "            \n",
    "            \n",
    "            # Add the pair to the primary info dictionary\n",
    "            primary_info[label] = value\n",
    "\n",
    "    building_data['Primary Info'] = primary_info\n",
    "\n",
    "    \n",
    "    # Scrape additional features and fixtures\n",
    "    additional_features = []\n",
    "    more_details = building_details.find_all('div', class_='skinny-row')\n",
    "    for detail in more_details:\n",
    "        name = detail.find('span', class_='name')\n",
    "        value = detail.find('span', class_='value')\n",
    "        if name and value:\n",
    "            name_text = name.text.strip().replace('\\n', ' ').replace('\\r', '')\n",
    "            value_text = value.text.strip().replace('\\n', ' ').replace('\\r', '')\n",
    "            additional_features.append({name_text: value_text})\n",
    "    \n",
    "    building_data['Additional Features'] = additional_features\n",
    "\n",
    "else:\n",
    "    print(\"Building details section not found.\")\n",
    "\n",
    "index = 0\n",
    "for item in building_data['Images']:\n",
    "    data_set3[\"image\"+str(index)] = item\n",
    "    index += 1\n",
    "\n",
    "index = 0\n",
    "for item in building_data['Additional Features']:\n",
    "    data_set3[\"feature\"+str(index)] = item\n",
    "    index += 1\n",
    "\n",
    "\n",
    "# Print the scraped data, each key-value pair on its own line\n",
    "# for key, value in building_data.items():\n",
    "#     print(f\"{key}:\")\n",
    "#     if isinstance(value, list):\n",
    "#         for item in value:\n",
    "#             # If the value is a dictionary (for additional features)\n",
    "#             if isinstance(item, dict): \n",
    "#                 for sub_key, sub_value in item.items():\n",
    "#                     print(f\"  {sub_key} {sub_value}\")\n",
    "#             else:\n",
    "#                 print(f\"  {item}\")\n",
    "#     else:\n",
    "#         # For primary info, print each label-value pair on a new line\n",
    "#         if key == '\\nPrimary Info': \n",
    "#             for label, value in value.items():\n",
    "#                 print(f\"  {label} {value}\")\n",
    "#         else:\n",
    "#             print(f\"  {value}\")\n",
    "\n",
    "# Close the browser after scraping\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "572e5460-f283-4057-a673-5ad9bf96a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "land_info = {}\n",
    "\n",
    "# Find the \"LandInfoAndValue\" section\n",
    "land_info_section = soup.find('div', {'id': 'LandInfoAndValue'})\n",
    "\n",
    "if land_info_section:\n",
    "    # Scrape Land Details (Land Type, Class Code, etc.)\n",
    "    land_details = land_info_section.find_all('div', class_='row')\n",
    "    \n",
    "    for detail in land_details:\n",
    "        label = detail.find('div', class_='col-xs-3')\n",
    "        value = detail.find('div', class_='col-xs-9')\n",
    "        \n",
    "        if label and value:\n",
    "            # Clean up label and value text\n",
    "            label_text = label.text.strip().replace('\\n', '').replace('\\r', '')\n",
    "            value_text = value.text.strip().replace('\\n', '').replace('\\r', '')\n",
    "            \n",
    "            # Add label-value pair to the dictionary\n",
    "            land_info[label_text] = value_text\n",
    "    \n",
    "    # Scrape Land Valuation (Actual Value)\n",
    "    valuation_section = land_info_section.find('div', class_='header')\n",
    "    if valuation_section and 'Land Valuation' in valuation_section.text:\n",
    "        # Last row is the valuation row\n",
    "        valuation_row = land_info_section.find_all('div', class_='row')[-1]  \n",
    "        actual_value_label = valuation_row.find('div', class_='col-sm-3')\n",
    "        actual_value = valuation_row.find('div', class_='col-sm-9')\n",
    "        \n",
    "        if actual_value_label and actual_value:\n",
    "            land_info['Actual Value'] = actual_value.text.strip().replace('\\n', '').replace('\\r', '')\n",
    "\n",
    "# Print the scraped land info\n",
    "for key, value in land_info.items():\n",
    "    data_set4[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d3da4ad-a4f5-4032-a2cb-cc86655457a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the table with class 'tax-data-table table'\n",
    "table = soup.find('table', class_='tax-data-table table')\n",
    "\n",
    "if table:\n",
    "    # Find all tbody elements inside the table with the 'tax-info' class (except the last 'total-row')\n",
    "    rows = table.find_all('tbody', class_='tax-info')  \n",
    "    \n",
    "    sales_data = []\n",
    "    for row in rows:\n",
    "        # Extract data from the first row\n",
    "        tax_id = row.find_all('td')[0].text.strip() if len(row.find_all('td')) > 0 else None\n",
    "        authority_name = row.find_all('td')[1].text.strip() if len(row.find_all('td')) > 1 else None\n",
    "        mills = row.find_all('td')[2].text.strip() if len(row.find_all('td')) > 2 else None\n",
    "        tax_rate = row.find_all('td')[3].text.strip() if len(row.find_all('td')) > 3 else None\n",
    "        tax_amount = row.find_all('td')[4].text.strip() if len(row.find_all('td')) > 4 else None\n",
    "        \n",
    "        # Store row data in a dictionary\n",
    "        sales_data.append({\n",
    "            'ID': tax_id,\n",
    "            'Authority Name': authority_name,\n",
    "            'Mills': mills,\n",
    "            'Tax Rate': tax_rate,\n",
    "            'Est. Tax Amount': tax_amount\n",
    "        })\n",
    "\n",
    "    index = 0\n",
    "    # Print extracted data\n",
    "    for row in sales_data:\n",
    "        data_set5[\"tax\"+str(index)] = row\n",
    "        index+=1\n",
    "        \n",
    "else:\n",
    "    print(\"Table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfb97017-c697-4e9d-805d-32d8e000bec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropdown_content = soup.find('div', id='Documents').find('div', class_='dropdown-content')\n",
    "temp_data_set = {}\n",
    "# Find the list of documents inside the dropdown\n",
    "documents_list = dropdown_content.find_all('li', class_='ng-star-inserted') if dropdown_content else []\n",
    "\n",
    "if documents_list:\n",
    "    # Initialize an empty dictionary to store document data\n",
    "    document_data = {}\n",
    "\n",
    "    # Iterate through each document item and extract the desired data\n",
    "    for doc in documents_list:\n",
    "        # Get document name (PDF filename)\n",
    "        doc_name = doc.find('a').text.strip() if doc.find('a') else None\n",
    "        \n",
    "        # Get file size\n",
    "        size = doc.find('div', class_='col-sm-2')\n",
    "        size = size.text.strip().replace('Size:', '').strip() if size else None\n",
    "        \n",
    "        # Get last modified date\n",
    "        last_modified = doc.find('div', class_='col-sm-4')\n",
    "        last_modified = last_modified.text.strip().replace('Last Modified Date:', '').strip() if last_modified else None\n",
    "        \n",
    "        # Store the extracted data in the dictionary with document name as the key\n",
    "        document_data[doc_name] = {\n",
    "            'Name': doc_name,\n",
    "            'Size': size,\n",
    "            'Last Modified Date': last_modified\n",
    "        }\n",
    "\n",
    "    # Print the document data dictionary\n",
    "    index = 0\n",
    "    for doc_name, details in document_data.items():\n",
    "        data_set6[\"document\"+str(index)] = details\n",
    "        index += 1\n",
    "        \n",
    "else:\n",
    "    print(\"No documents found \")\n",
    "\n",
    "# Close the browser\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3c3ed3d-192d-4f47-bd35-1eb41017e3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10a5443d-0050-4cb8-b08d-aedc143fa5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [05/Mar/2025 20:59:40] \"GET /option3 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Mar/2025 20:59:40] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "# ‘/’ URL is bound with hello_world() function.\n",
    "def option1():\n",
    "    house_info = []\n",
    "    returnval = {}\n",
    "    house_info.append({\n",
    "                    \"Year\": 2020,\n",
    "                    \"Type\": \"property_type\",\n",
    "                    \"Actual\": \"actual\",\n",
    "                    \"Assessed\": \"assessed\",\n",
    "                    \"Exempt\": \"exempt\"\n",
    "                })\n",
    "    house_info.append({\n",
    "                    \"Year\": 2021,\n",
    "                    \"Type\": \"property_type\",\n",
    "                    \"Actual\": \"actual\",\n",
    "                    \"Assessed\": \"assessed\",\n",
    "                    \"Exempt\": \"exempt\"\n",
    "                })\n",
    "    \n",
    "    index = 0\n",
    "    for item in house_info:\n",
    "        returnval[\"detail\"+str(index)] = item\n",
    "        index += 1\n",
    "    return jsonify(returnval)\n",
    "\n",
    "@app.route('/option2')\n",
    "# ‘/’ URL is bound with hello_world() function.\n",
    "def option2():\n",
    "    data_set = {}\n",
    "    data_set[\"Owner\"] = \"Zach\"\n",
    "    data_set[\"Address\"] = \"3135 Moorhead Ave\"\n",
    "    data_set[\"Price1\"] = \"$20,000\"\n",
    "    data_set[\"Year1\"] = 1960\n",
    "    data_set[\"Price2\"] = \"$500,000\"\n",
    "    data_set[\"Year2\"] = 2020\n",
    "    return data_set\n",
    "\n",
    "@app.route('/option3')\n",
    "# ‘/’ URL is bound with hello_world() function.\n",
    "def option3():\n",
    "    return jsonify(data_set7)\n",
    "    \n",
    "# main driver function\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # run() method of Flask class runs the application \n",
    "    # on the local development server.\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8193d5-7a92-487d-bbf9-a2a1acfc03a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb78b284-33f5-4ded-b580-9a11a0cb96ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f70e97-22b1-417c-be85-e1c1150dc46d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
